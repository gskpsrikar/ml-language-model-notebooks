{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a4eef3",
   "metadata": {},
   "source": [
    "# Keras Text Classification Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc8493",
   "metadata": {},
   "source": [
    "###### Purpose of this notebook: \n",
    "\n",
    "CONTEXT: Text Classification Models\n",
    "\n",
    "Whenever Neural Networks are build, deploying them as inference endpoints would require some preprocessing steps on the new input and also the model file. The purpose of this notebook is to get all the artifacts of the Neural Network Model needed to do inference on an new text input.\n",
    "\n",
    "###### Needed:\n",
    "1. The model file (Example: The .h5 file)\n",
    "2. Tokenizer (to convert text to tokens in the same way)\n",
    "3. Other params\n",
    "\n",
    "###### Priorities\n",
    "1. The NN model need not be the best.\n",
    "2. The goal is to develop an inference pipeline on Amazon Web Services. For prototyping, I chose a small dataset from Kaggle (diasaster tweets) and a model with as few parameters as possible.\n",
    "3. The accuracy is not expected to be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4603f6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb92ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, SpatialDropout1D, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras_transformer import get_model, get_custom_objects\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a80adf",
   "metadata": {},
   "source": [
    "## Save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf66e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"path/to/data\"\n",
    "df = pd.read_csv(PATH, usecols=['text', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \n",
    "    tweet = tweet.lower() # convert to lowercase\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet) # remove urls\n",
    "    tweet = re.sub(r\"@\\w+\", \"\", tweet) # remove mentions\n",
    "    tweet = re.sub(r\"#\\w+\", \"\", tweet) # remove hashtags\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # remove punctuation\n",
    "    tweet = re.sub(r'\\d+', '', tweet) # remove numbers\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip() # remove extra whitespace\n",
    "    \n",
    "    stopwords = set([\n",
    "        'ourselves', 'hers','between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very',\n",
    "        'having', 'with', 'they', 'own', 'an', 'be', 'some','for', 'do', 'its', 'yours', 'such', 'into', 'of', \n",
    "        'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves',\n",
    "        'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\n",
    "        'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she',\n",
    "        'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does',\n",
    "        'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he',\n",
    "        'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom',\n",
    "        't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'\n",
    "    ])\n",
    "    tweet = tweet.split()\n",
    "    tweet = [i for i in tweet if i not in stopwords]\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc377e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['text'].tolist())\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1aea5",
   "metadata": {},
   "source": [
    "### Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e32a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disaster-tweet-tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6534d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pad_sequences(sequences, padding='post', maxlen=16)\n",
    "y = np.array(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c379981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, max_len):\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "\n",
    "    x = Embedding(vocab_size, 4)(input_layer)\n",
    "    x = SpatialDropout1D(rate=0.1)(x)\n",
    "    x = LSTM(32, return_sequences=True, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = LSTM(16, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "    model = Model(input_layer, x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(5000, 16)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e88728",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_split=0.15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1467fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aeef2a",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebcffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath='disaster-tweet-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded580c",
   "metadata": {},
   "source": [
    "### Save modelling params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fbb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"padding_type\": \"post\",\n",
    "    \"maxlen\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disaster-tweet-modelling-params.pkl', 'wb') as f:\n",
    "    pickle.dump(params, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42741cfe",
   "metadata": {},
   "source": [
    "## Load Model, Tokenizer and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33334a9c",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"disaster-tweet-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb948383",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf179193",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disaster-tweet-tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55024b69",
   "metadata": {},
   "source": [
    "### Load modelling params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6da532",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disaster-tweet-modelling-params.pkl', 'rb') as handle:\n",
    "    params = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec028812",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"no disaster warning; no need to seek shelter immediately.\"\n",
    "new_text = preprocess_tweet(new_text)\n",
    "\n",
    "tokenized_text = tokenizer.texts_to_sequences([new_text])\n",
    "tokenized_text = pad_sequences(tokenized_text, maxlen=16, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(tokenized_text, np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab838e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
